2022-11-11 16:25:37,419 - mmcls - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.13 (default, Oct 21 2022, 23:50:54) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU
CUDA_HOME: /usr/local/cuda-11.1
NVCC: Cuda compilation tools, release 11.1, V11.1.105
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.13.0
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0
OpenCV: 4.6.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.7
MMClassification: 0.24.1+HEAD
------------------------------------------------------------

2022-11-11 16:25:37,419 - mmcls - INFO - Distributed training: False
2022-11-11 16:25:37,464 - mmcls - INFO - Config:
model = dict(
    type='BSConvClassifier',
    backbone=dict(
        type='MobileNetV3Cifar', arch='large', conv_cfg=dict(type='BSConvS')),
    neck=dict(type='GlobalAveragePooling'),
    head=dict(
        type='StackedLinearClsHead',
        num_classes=100,
        in_channels=960,
        mid_channels=[1280],
        act_cfg=dict(type='HSwish'),
        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
        topk=(1, 5)))
dataset_type = 'CIFAR100'
img_norm_cfg = dict(
    mean=[129.304, 124.07, 112.434], std=[68.17, 65.392, 70.418], to_rgb=False)
train_pipeline = [
    dict(type='RandomCrop', size=32, padding=4),
    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
    dict(
        type='Normalize',
        mean=[129.304, 124.07, 112.434],
        std=[68.17, 65.392, 70.418],
        to_rgb=False),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='ToTensor', keys=['gt_label']),
    dict(type='Collect', keys=['img', 'gt_label'])
]
test_pipeline = [
    dict(
        type='Normalize',
        mean=[129.304, 124.07, 112.434],
        std=[68.17, 65.392, 70.418],
        to_rgb=False),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='Collect', keys=['img'])
]
data = dict(
    samples_per_gpu=128,
    workers_per_gpu=2,
    train=dict(
        type='CIFAR100',
        data_prefix='data/cifar100',
        pipeline=[
            dict(type='RandomCrop', size=32, padding=4),
            dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
            dict(
                type='Normalize',
                mean=[129.304, 124.07, 112.434],
                std=[68.17, 65.392, 70.418],
                to_rgb=False),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='ToTensor', keys=['gt_label']),
            dict(type='Collect', keys=['img', 'gt_label'])
        ]),
    val=dict(
        type='CIFAR100',
        data_prefix='data/cifar100',
        pipeline=[
            dict(
                type='Normalize',
                mean=[129.304, 124.07, 112.434],
                std=[68.17, 65.392, 70.418],
                to_rgb=False),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ],
        test_mode=True),
    test=dict(
        type='CIFAR100',
        data_prefix='data/cifar100',
        pipeline=[
            dict(
                type='Normalize',
                mean=[129.304, 124.07, 112.434],
                std=[68.17, 65.392, 70.418],
                to_rgb=False),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ],
        test_mode=True))
optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(policy='step', step=[100, 150, 180])
runner = dict(type='EpochBasedRunner', max_epochs=200)
checkpoint_config = dict(interval=10, max_keep_ckpts=1)
log_config = dict(
    interval=100,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
work_dir = './work_dirs/bsconvs_mobilenet_v3_large_b128_cifar100'
gpu_ids = [0]

2022-11-11 16:25:37,464 - mmcls - INFO - Set random seed to 630735287, deterministic: False
2022-11-11 16:25:37,528 - mmcls - INFO - initialize MobileNetV3Cifar with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d'], 'nonlinearity': 'leaky_relu'}, {'type': 'Normal', 'layer': ['Linear'], 'std': 0.01}, {'type': 'Constant', 'layer': ['BatchNorm2d'], 'val': 1}]
Name of parameter - Initialization information

backbone.layer0.conv.pw1.weight - torch.Size([3, 3, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer0.conv.pw2.weight - torch.Size([16, 3, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer0.conv.dw.weight - torch.Size([16, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer0.bn.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer0.bn.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer1.depthwise_conv.conv.pw1.weight - torch.Size([4, 16, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer1.depthwise_conv.conv.pw2.weight - torch.Size([16, 4, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer1.depthwise_conv.conv.dw.weight - torch.Size([16, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer1.depthwise_conv.bn.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer1.depthwise_conv.bn.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer1.linear_conv.conv.pw1.weight - torch.Size([4, 16, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer1.linear_conv.conv.pw2.weight - torch.Size([16, 4, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer1.linear_conv.conv.dw.weight - torch.Size([16, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer1.linear_conv.bn.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer1.linear_conv.bn.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer2.expand_conv.conv.pw1.weight - torch.Size([4, 16, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.expand_conv.conv.pw2.weight - torch.Size([64, 4, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.expand_conv.conv.dw.weight - torch.Size([64, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.expand_conv.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer2.expand_conv.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer2.depthwise_conv.conv.pw1.weight - torch.Size([16, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.depthwise_conv.conv.pw2.weight - torch.Size([64, 16, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.depthwise_conv.conv.dw.weight - torch.Size([64, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.depthwise_conv.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer2.depthwise_conv.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer2.linear_conv.conv.pw1.weight - torch.Size([16, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.linear_conv.conv.pw2.weight - torch.Size([24, 16, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.linear_conv.conv.dw.weight - torch.Size([24, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer2.linear_conv.bn.weight - torch.Size([24]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer2.linear_conv.bn.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer3.expand_conv.conv.pw1.weight - torch.Size([6, 24, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.expand_conv.conv.pw2.weight - torch.Size([72, 6, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.expand_conv.conv.dw.weight - torch.Size([72, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.expand_conv.bn.weight - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer3.expand_conv.bn.bias - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer3.depthwise_conv.conv.pw1.weight - torch.Size([18, 72, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.depthwise_conv.conv.pw2.weight - torch.Size([72, 18, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.depthwise_conv.conv.dw.weight - torch.Size([72, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.depthwise_conv.bn.weight - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer3.depthwise_conv.bn.bias - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer3.linear_conv.conv.pw1.weight - torch.Size([18, 72, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.linear_conv.conv.pw2.weight - torch.Size([24, 18, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.linear_conv.conv.dw.weight - torch.Size([24, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer3.linear_conv.bn.weight - torch.Size([24]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer3.linear_conv.bn.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.expand_conv.conv.pw1.weight - torch.Size([6, 24, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.expand_conv.conv.pw2.weight - torch.Size([72, 6, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.expand_conv.conv.dw.weight - torch.Size([72, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.expand_conv.bn.weight - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.expand_conv.bn.bias - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.depthwise_conv.conv.pw1.weight - torch.Size([18, 72, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.depthwise_conv.conv.pw2.weight - torch.Size([72, 18, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.depthwise_conv.conv.dw.weight - torch.Size([72, 1, 5, 5]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.depthwise_conv.bn.weight - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.depthwise_conv.bn.bias - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.se.conv1.conv.weight - torch.Size([24, 72, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer4.se.conv1.conv.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.se.conv2.conv.weight - torch.Size([72, 24, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer4.se.conv2.conv.bias - torch.Size([72]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.linear_conv.conv.pw1.weight - torch.Size([18, 72, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.linear_conv.conv.pw2.weight - torch.Size([40, 18, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.linear_conv.conv.dw.weight - torch.Size([40, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer4.linear_conv.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer4.linear_conv.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.expand_conv.conv.pw1.weight - torch.Size([10, 40, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.expand_conv.conv.pw2.weight - torch.Size([120, 10, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.expand_conv.conv.dw.weight - torch.Size([120, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.expand_conv.bn.weight - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.expand_conv.bn.bias - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.depthwise_conv.conv.pw1.weight - torch.Size([30, 120, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.depthwise_conv.conv.pw2.weight - torch.Size([120, 30, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.depthwise_conv.conv.dw.weight - torch.Size([120, 1, 5, 5]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.depthwise_conv.bn.weight - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.depthwise_conv.bn.bias - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.se.conv1.conv.weight - torch.Size([32, 120, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer5.se.conv1.conv.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.se.conv2.conv.weight - torch.Size([120, 32, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer5.se.conv2.conv.bias - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.linear_conv.conv.pw1.weight - torch.Size([30, 120, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.linear_conv.conv.pw2.weight - torch.Size([40, 30, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.linear_conv.conv.dw.weight - torch.Size([40, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer5.linear_conv.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer5.linear_conv.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.expand_conv.conv.pw1.weight - torch.Size([10, 40, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.expand_conv.conv.pw2.weight - torch.Size([120, 10, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.expand_conv.conv.dw.weight - torch.Size([120, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.expand_conv.bn.weight - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.expand_conv.bn.bias - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.depthwise_conv.conv.pw1.weight - torch.Size([30, 120, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.depthwise_conv.conv.pw2.weight - torch.Size([120, 30, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.depthwise_conv.conv.dw.weight - torch.Size([120, 1, 5, 5]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.depthwise_conv.bn.weight - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.depthwise_conv.bn.bias - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.se.conv1.conv.weight - torch.Size([32, 120, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer6.se.conv1.conv.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.se.conv2.conv.weight - torch.Size([120, 32, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer6.se.conv2.conv.bias - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.linear_conv.conv.pw1.weight - torch.Size([30, 120, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.linear_conv.conv.pw2.weight - torch.Size([40, 30, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.linear_conv.conv.dw.weight - torch.Size([40, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer6.linear_conv.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer6.linear_conv.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer7.expand_conv.conv.pw1.weight - torch.Size([10, 40, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.expand_conv.conv.pw2.weight - torch.Size([240, 10, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.expand_conv.conv.dw.weight - torch.Size([240, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.expand_conv.bn.weight - torch.Size([240]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer7.expand_conv.bn.bias - torch.Size([240]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer7.depthwise_conv.conv.pw1.weight - torch.Size([60, 240, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.depthwise_conv.conv.pw2.weight - torch.Size([240, 60, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.depthwise_conv.conv.dw.weight - torch.Size([240, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.depthwise_conv.bn.weight - torch.Size([240]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer7.depthwise_conv.bn.bias - torch.Size([240]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer7.linear_conv.conv.pw1.weight - torch.Size([60, 240, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.linear_conv.conv.pw2.weight - torch.Size([80, 60, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.linear_conv.conv.dw.weight - torch.Size([80, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer7.linear_conv.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer7.linear_conv.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer8.expand_conv.conv.pw1.weight - torch.Size([20, 80, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.expand_conv.conv.pw2.weight - torch.Size([200, 20, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.expand_conv.conv.dw.weight - torch.Size([200, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.expand_conv.bn.weight - torch.Size([200]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer8.expand_conv.bn.bias - torch.Size([200]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer8.depthwise_conv.conv.pw1.weight - torch.Size([50, 200, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.depthwise_conv.conv.pw2.weight - torch.Size([200, 50, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.depthwise_conv.conv.dw.weight - torch.Size([200, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.depthwise_conv.bn.weight - torch.Size([200]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer8.depthwise_conv.bn.bias - torch.Size([200]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer8.linear_conv.conv.pw1.weight - torch.Size([50, 200, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.linear_conv.conv.pw2.weight - torch.Size([80, 50, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.linear_conv.conv.dw.weight - torch.Size([80, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer8.linear_conv.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer8.linear_conv.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer9.expand_conv.conv.pw1.weight - torch.Size([20, 80, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.expand_conv.conv.pw2.weight - torch.Size([184, 20, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.expand_conv.conv.dw.weight - torch.Size([184, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.expand_conv.bn.weight - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer9.expand_conv.bn.bias - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer9.depthwise_conv.conv.pw1.weight - torch.Size([46, 184, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.depthwise_conv.conv.pw2.weight - torch.Size([184, 46, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.depthwise_conv.conv.dw.weight - torch.Size([184, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.depthwise_conv.bn.weight - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer9.depthwise_conv.bn.bias - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer9.linear_conv.conv.pw1.weight - torch.Size([46, 184, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.linear_conv.conv.pw2.weight - torch.Size([80, 46, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.linear_conv.conv.dw.weight - torch.Size([80, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer9.linear_conv.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer9.linear_conv.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer10.expand_conv.conv.pw1.weight - torch.Size([20, 80, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.expand_conv.conv.pw2.weight - torch.Size([184, 20, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.expand_conv.conv.dw.weight - torch.Size([184, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.expand_conv.bn.weight - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer10.expand_conv.bn.bias - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer10.depthwise_conv.conv.pw1.weight - torch.Size([46, 184, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.depthwise_conv.conv.pw2.weight - torch.Size([184, 46, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.depthwise_conv.conv.dw.weight - torch.Size([184, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.depthwise_conv.bn.weight - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer10.depthwise_conv.bn.bias - torch.Size([184]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer10.linear_conv.conv.pw1.weight - torch.Size([46, 184, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.linear_conv.conv.pw2.weight - torch.Size([80, 46, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.linear_conv.conv.dw.weight - torch.Size([80, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer10.linear_conv.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer10.linear_conv.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.expand_conv.conv.pw1.weight - torch.Size([20, 80, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.expand_conv.conv.pw2.weight - torch.Size([480, 20, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.expand_conv.conv.dw.weight - torch.Size([480, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.expand_conv.bn.weight - torch.Size([480]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.expand_conv.bn.bias - torch.Size([480]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.depthwise_conv.conv.pw1.weight - torch.Size([120, 480, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.depthwise_conv.conv.pw2.weight - torch.Size([480, 120, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.depthwise_conv.conv.dw.weight - torch.Size([480, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.depthwise_conv.bn.weight - torch.Size([480]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.depthwise_conv.bn.bias - torch.Size([480]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.se.conv1.conv.weight - torch.Size([120, 480, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer11.se.conv1.conv.bias - torch.Size([120]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.se.conv2.conv.weight - torch.Size([480, 120, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer11.se.conv2.conv.bias - torch.Size([480]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.linear_conv.conv.pw1.weight - torch.Size([120, 480, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.linear_conv.conv.pw2.weight - torch.Size([112, 120, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.linear_conv.conv.dw.weight - torch.Size([112, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer11.linear_conv.bn.weight - torch.Size([112]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer11.linear_conv.bn.bias - torch.Size([112]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.expand_conv.conv.pw1.weight - torch.Size([28, 112, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.expand_conv.conv.pw2.weight - torch.Size([672, 28, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.expand_conv.conv.dw.weight - torch.Size([672, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.expand_conv.bn.weight - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.expand_conv.bn.bias - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.depthwise_conv.conv.pw1.weight - torch.Size([168, 672, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.depthwise_conv.conv.pw2.weight - torch.Size([672, 168, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.depthwise_conv.conv.dw.weight - torch.Size([672, 1, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.depthwise_conv.bn.weight - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.depthwise_conv.bn.bias - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.se.conv1.conv.weight - torch.Size([168, 672, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer12.se.conv1.conv.bias - torch.Size([168]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.se.conv2.conv.weight - torch.Size([672, 168, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer12.se.conv2.conv.bias - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.linear_conv.conv.pw1.weight - torch.Size([168, 672, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.linear_conv.conv.pw2.weight - torch.Size([112, 168, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.linear_conv.conv.dw.weight - torch.Size([112, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer12.linear_conv.bn.weight - torch.Size([112]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer12.linear_conv.bn.bias - torch.Size([112]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.expand_conv.conv.pw1.weight - torch.Size([28, 112, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.expand_conv.conv.pw2.weight - torch.Size([672, 28, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.expand_conv.conv.dw.weight - torch.Size([672, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.expand_conv.bn.weight - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.expand_conv.bn.bias - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.depthwise_conv.conv.pw1.weight - torch.Size([168, 672, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.depthwise_conv.conv.pw2.weight - torch.Size([672, 168, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.depthwise_conv.conv.dw.weight - torch.Size([672, 1, 5, 5]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.depthwise_conv.bn.weight - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.depthwise_conv.bn.bias - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.se.conv1.conv.weight - torch.Size([168, 672, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer13.se.conv1.conv.bias - torch.Size([168]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.se.conv2.conv.weight - torch.Size([672, 168, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer13.se.conv2.conv.bias - torch.Size([672]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.linear_conv.conv.pw1.weight - torch.Size([168, 672, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.linear_conv.conv.pw2.weight - torch.Size([160, 168, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.linear_conv.conv.dw.weight - torch.Size([160, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer13.linear_conv.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer13.linear_conv.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.expand_conv.conv.pw1.weight - torch.Size([40, 160, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.expand_conv.conv.pw2.weight - torch.Size([960, 40, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.expand_conv.conv.dw.weight - torch.Size([960, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.expand_conv.bn.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.expand_conv.bn.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.depthwise_conv.conv.pw1.weight - torch.Size([240, 960, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.depthwise_conv.conv.pw2.weight - torch.Size([960, 240, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.depthwise_conv.conv.dw.weight - torch.Size([960, 1, 5, 5]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.depthwise_conv.bn.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.depthwise_conv.bn.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.se.conv1.conv.weight - torch.Size([240, 960, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer14.se.conv1.conv.bias - torch.Size([240]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.se.conv2.conv.weight - torch.Size([960, 240, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer14.se.conv2.conv.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.linear_conv.conv.pw1.weight - torch.Size([240, 960, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.linear_conv.conv.pw2.weight - torch.Size([160, 240, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.linear_conv.conv.dw.weight - torch.Size([160, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer14.linear_conv.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer14.linear_conv.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.expand_conv.conv.pw1.weight - torch.Size([40, 160, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.expand_conv.conv.pw2.weight - torch.Size([960, 40, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.expand_conv.conv.dw.weight - torch.Size([960, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.expand_conv.bn.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.expand_conv.bn.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.depthwise_conv.conv.pw1.weight - torch.Size([240, 960, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.depthwise_conv.conv.pw2.weight - torch.Size([960, 240, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.depthwise_conv.conv.dw.weight - torch.Size([960, 1, 5, 5]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.depthwise_conv.bn.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.depthwise_conv.bn.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.se.conv1.conv.weight - torch.Size([240, 960, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer15.se.conv1.conv.bias - torch.Size([240]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.se.conv2.conv.weight - torch.Size([960, 240, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.layer15.se.conv2.conv.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.linear_conv.conv.pw1.weight - torch.Size([240, 960, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.linear_conv.conv.pw2.weight - torch.Size([160, 240, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.linear_conv.conv.dw.weight - torch.Size([160, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer15.linear_conv.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer15.linear_conv.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer16.conv.pw1.weight - torch.Size([40, 160, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer16.conv.pw2.weight - torch.Size([960, 40, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer16.conv.dw.weight - torch.Size([960, 1, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=leaky_relu, distribution =normal, bias=0 

backbone.layer16.bn.weight - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

backbone.layer16.bn.bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

head.layers.0.fc.weight - torch.Size([1280, 960]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

head.layers.0.fc.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

head.layers.1.fc.weight - torch.Size([100, 1280]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  

head.layers.1.fc.bias - torch.Size([100]): 
The value is the same before and after calling `init_weights` of BSConvClassifier  
2022-11-11 16:25:39,295 - mmcls - INFO - Start running, host: aoyuli@LAY-LAPTOP, work_dir: /home/aoyuli/Project/mmdl/work_dirs/bsconvs_mobilenet_v3_large_b128_cifar100
2022-11-11 16:25:39,296 - mmcls - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-11-11 16:25:39,296 - mmcls - INFO - workflow: [('train', 1)], max: 200 epochs
2022-11-11 16:25:39,296 - mmcls - INFO - Checkpoints will be saved to /home/aoyuli/Project/mmdl/work_dirs/bsconvs_mobilenet_v3_large_b128_cifar100 by HardDiskBackend.
2022-11-11 16:25:54,893 - mmcls - INFO - Epoch [1][100/391]	lr: 1.000e-01, eta: 3:22:37, time: 0.156, data_time: 0.021, memory: 2300, loss: 4.4505
2022-11-11 16:26:06,869 - mmcls - INFO - Epoch [1][200/391]	lr: 1.000e-01, eta: 2:58:59, time: 0.120, data_time: 0.000, memory: 2300, loss: 4.2389
2022-11-11 16:26:19,002 - mmcls - INFO - Epoch [1][300/391]	lr: 1.000e-01, eta: 2:51:40, time: 0.121, data_time: 0.000, memory: 2300, loss: 4.0858
2022-11-11 16:26:32,898 - mmcls - INFO - Epoch(val) [1][79]	accuracy_top-1: 1.0000, accuracy_top-5: 5.0000
2022-11-11 16:26:47,060 - mmcls - INFO - Epoch [2][100/391]	lr: 1.000e-01, eta: 2:21:57, time: 0.141, data_time: 0.021, memory: 2300, loss: 3.9160
2022-11-11 16:26:59,565 - mmcls - INFO - Epoch [2][200/391]	lr: 1.000e-01, eta: 2:25:10, time: 0.125, data_time: 0.000, memory: 2300, loss: 3.8443
2022-11-11 16:27:12,378 - mmcls - INFO - Epoch [2][300/391]	lr: 1.000e-01, eta: 2:27:56, time: 0.128, data_time: 0.000, memory: 2300, loss: 3.7363
2022-11-11 16:27:26,631 - mmcls - INFO - Epoch(val) [2][79]	accuracy_top-1: 12.1900, accuracy_top-5: 36.2900
2022-11-11 16:27:41,116 - mmcls - INFO - Epoch [3][100/391]	lr: 1.000e-01, eta: 2:16:45, time: 0.145, data_time: 0.021, memory: 2300, loss: 3.6183
2022-11-11 16:27:53,825 - mmcls - INFO - Epoch [3][200/391]	lr: 1.000e-01, eta: 2:19:19, time: 0.127, data_time: 0.000, memory: 2300, loss: 3.5074
2022-11-11 16:28:06,982 - mmcls - INFO - Epoch [3][300/391]	lr: 1.000e-01, eta: 2:21:55, time: 0.132, data_time: 0.000, memory: 2300, loss: 3.5011
2022-11-11 16:28:21,714 - mmcls - INFO - Epoch(val) [3][79]	accuracy_top-1: 11.6100, accuracy_top-5: 32.5500
2022-11-11 16:28:36,738 - mmcls - INFO - Epoch [4][100/391]	lr: 1.000e-01, eta: 2:15:26, time: 0.150, data_time: 0.020, memory: 2300, loss: 3.3425
2022-11-11 16:28:50,232 - mmcls - INFO - Epoch [4][200/391]	lr: 1.000e-01, eta: 2:18:00, time: 0.135, data_time: 0.000, memory: 2300, loss: 3.2566
2022-11-11 16:29:03,148 - mmcls - INFO - Epoch [4][300/391]	lr: 1.000e-01, eta: 2:19:40, time: 0.129, data_time: 0.000, memory: 2300, loss: 3.2161
2022-11-11 16:29:17,352 - mmcls - INFO - Epoch(val) [4][79]	accuracy_top-1: 16.3600, accuracy_top-5: 42.2300
2022-11-11 16:29:31,922 - mmcls - INFO - Epoch [5][100/391]	lr: 1.000e-01, eta: 2:14:29, time: 0.146, data_time: 0.021, memory: 2300, loss: 3.0782
2022-11-11 16:29:44,630 - mmcls - INFO - Epoch [5][200/391]	lr: 1.000e-01, eta: 2:15:52, time: 0.127, data_time: 0.000, memory: 2300, loss: 3.0152
2022-11-11 16:29:57,682 - mmcls - INFO - Epoch [5][300/391]	lr: 1.000e-01, eta: 2:17:19, time: 0.131, data_time: 0.001, memory: 2300, loss: 2.9602
2022-11-11 16:30:12,233 - mmcls - INFO - Epoch(val) [5][79]	accuracy_top-1: 19.1000, accuracy_top-5: 46.3400
2022-11-11 16:30:27,299 - mmcls - INFO - Epoch [6][100/391]	lr: 1.000e-01, eta: 2:13:33, time: 0.151, data_time: 0.021, memory: 2300, loss: 2.8769
2022-11-11 16:30:40,489 - mmcls - INFO - Epoch [6][200/391]	lr: 1.000e-01, eta: 2:14:56, time: 0.132, data_time: 0.000, memory: 2300, loss: 2.8222
2022-11-11 16:30:53,335 - mmcls - INFO - Epoch [6][300/391]	lr: 1.000e-01, eta: 2:16:00, time: 0.128, data_time: 0.000, memory: 2300, loss: 2.8031
2022-11-11 16:31:07,793 - mmcls - INFO - Epoch(val) [6][79]	accuracy_top-1: 23.9600, accuracy_top-5: 54.7500
2022-11-11 16:31:22,610 - mmcls - INFO - Epoch [7][100/391]	lr: 1.000e-01, eta: 2:12:42, time: 0.148, data_time: 0.021, memory: 2300, loss: 2.6901
2022-11-11 16:31:35,638 - mmcls - INFO - Epoch [7][200/391]	lr: 1.000e-01, eta: 2:13:46, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.6712
2022-11-11 16:31:48,791 - mmcls - INFO - Epoch [7][300/391]	lr: 1.000e-01, eta: 2:14:48, time: 0.132, data_time: 0.000, memory: 2300, loss: 2.6498
2022-11-11 16:32:03,036 - mmcls - INFO - Epoch(val) [7][79]	accuracy_top-1: 22.0900, accuracy_top-5: 50.5400
2022-11-11 16:32:18,000 - mmcls - INFO - Epoch [8][100/391]	lr: 1.000e-01, eta: 2:12:02, time: 0.150, data_time: 0.021, memory: 2300, loss: 2.5620
2022-11-11 16:32:31,024 - mmcls - INFO - Epoch [8][200/391]	lr: 1.000e-01, eta: 2:12:56, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.5530
2022-11-11 16:32:44,075 - mmcls - INFO - Epoch [8][300/391]	lr: 1.000e-01, eta: 2:13:46, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.5449
2022-11-11 16:32:58,556 - mmcls - INFO - Epoch(val) [8][79]	accuracy_top-1: 21.8800, accuracy_top-5: 50.3300
2022-11-11 16:33:13,149 - mmcls - INFO - Epoch [9][100/391]	lr: 1.000e-01, eta: 2:11:10, time: 0.146, data_time: 0.021, memory: 2300, loss: 2.4795
2022-11-11 16:33:26,062 - mmcls - INFO - Epoch [9][200/391]	lr: 1.000e-01, eta: 2:11:54, time: 0.129, data_time: 0.000, memory: 2300, loss: 2.4611
2022-11-11 16:33:39,029 - mmcls - INFO - Epoch [9][300/391]	lr: 1.000e-01, eta: 2:12:36, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.4536
2022-11-11 16:33:53,565 - mmcls - INFO - Epoch(val) [9][79]	accuracy_top-1: 32.5500, accuracy_top-5: 65.5500
2022-11-11 16:34:08,745 - mmcls - INFO - Epoch [10][100/391]	lr: 1.000e-01, eta: 2:10:29, time: 0.152, data_time: 0.021, memory: 2300, loss: 2.3832
2022-11-11 16:34:21,870 - mmcls - INFO - Epoch [10][200/391]	lr: 1.000e-01, eta: 2:11:11, time: 0.131, data_time: 0.000, memory: 2300, loss: 2.3825
2022-11-11 16:34:34,866 - mmcls - INFO - Epoch [10][300/391]	lr: 1.000e-01, eta: 2:11:48, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.3496
2022-11-11 16:34:46,617 - mmcls - INFO - Saving checkpoint at 10 epochs
2022-11-11 16:34:49,319 - mmcls - INFO - Epoch(val) [10][79]	accuracy_top-1: 20.3200, accuracy_top-5: 47.2500
2022-11-11 16:35:04,159 - mmcls - INFO - Epoch [11][100/391]	lr: 1.000e-01, eta: 2:09:46, time: 0.148, data_time: 0.021, memory: 2300, loss: 2.3391
2022-11-11 16:35:17,162 - mmcls - INFO - Epoch [11][200/391]	lr: 1.000e-01, eta: 2:10:21, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.3047
2022-11-11 16:35:30,249 - mmcls - INFO - Epoch [11][300/391]	lr: 1.000e-01, eta: 2:10:55, time: 0.131, data_time: 0.000, memory: 2300, loss: 2.2848
2022-11-11 16:35:44,804 - mmcls - INFO - Epoch(val) [11][79]	accuracy_top-1: 30.8000, accuracy_top-5: 62.9300
2022-11-11 16:35:59,955 - mmcls - INFO - Epoch [12][100/391]	lr: 1.000e-01, eta: 2:09:09, time: 0.151, data_time: 0.021, memory: 2300, loss: 2.2378
2022-11-11 16:36:13,209 - mmcls - INFO - Epoch [12][200/391]	lr: 1.000e-01, eta: 2:09:43, time: 0.132, data_time: 0.000, memory: 2300, loss: 2.2559
2022-11-11 16:36:26,200 - mmcls - INFO - Epoch [12][300/391]	lr: 1.000e-01, eta: 2:10:11, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.2601
2022-11-11 16:36:40,736 - mmcls - INFO - Epoch(val) [12][79]	accuracy_top-1: 32.3400, accuracy_top-5: 65.6400
2022-11-11 16:36:55,751 - mmcls - INFO - Epoch [13][100/391]	lr: 1.000e-01, eta: 2:08:30, time: 0.150, data_time: 0.021, memory: 2300, loss: 2.1882
2022-11-11 16:37:08,693 - mmcls - INFO - Epoch [13][200/391]	lr: 1.000e-01, eta: 2:08:57, time: 0.129, data_time: 0.000, memory: 2300, loss: 2.1630
2022-11-11 16:37:21,599 - mmcls - INFO - Epoch [13][300/391]	lr: 1.000e-01, eta: 2:09:20, time: 0.129, data_time: 0.000, memory: 2300, loss: 2.2029
2022-11-11 16:37:35,892 - mmcls - INFO - Epoch(val) [13][79]	accuracy_top-1: 32.9000, accuracy_top-5: 63.1800
2022-11-11 16:37:50,818 - mmcls - INFO - Epoch [14][100/391]	lr: 1.000e-01, eta: 2:07:45, time: 0.149, data_time: 0.021, memory: 2300, loss: 2.1422
2022-11-11 16:38:04,260 - mmcls - INFO - Epoch [14][200/391]	lr: 1.000e-01, eta: 2:08:15, time: 0.134, data_time: 0.000, memory: 2300, loss: 2.1494
2022-11-11 16:38:18,211 - mmcls - INFO - Epoch [14][300/391]	lr: 1.000e-01, eta: 2:08:51, time: 0.139, data_time: 0.000, memory: 2300, loss: 2.1274
2022-11-11 16:38:33,112 - mmcls - INFO - Epoch(val) [14][79]	accuracy_top-1: 29.9500, accuracy_top-5: 61.0100
2022-11-11 16:38:48,097 - mmcls - INFO - Epoch [15][100/391]	lr: 1.000e-01, eta: 2:07:21, time: 0.150, data_time: 0.021, memory: 2300, loss: 2.0792
2022-11-11 16:39:01,217 - mmcls - INFO - Epoch [15][200/391]	lr: 1.000e-01, eta: 2:07:44, time: 0.131, data_time: 0.000, memory: 2300, loss: 2.0863
2022-11-11 16:39:14,199 - mmcls - INFO - Epoch [15][300/391]	lr: 1.000e-01, eta: 2:08:04, time: 0.130, data_time: 0.000, memory: 2300, loss: 2.0946
2022-11-11 16:39:28,623 - mmcls - INFO - Epoch(val) [15][79]	accuracy_top-1: 24.9600, accuracy_top-5: 53.3400
2022-11-11 16:39:43,232 - mmcls - INFO - Epoch [16][100/391]	lr: 1.000e-01, eta: 2:06:35, time: 0.146, data_time: 0.021, memory: 2300, loss: 2.0456
2022-11-11 16:39:55,903 - mmcls - INFO - Epoch [16][200/391]	lr: 1.000e-01, eta: 2:06:50, time: 0.127, data_time: 0.000, memory: 2300, loss: 2.0947
2022-11-11 16:40:09,168 - mmcls - INFO - Epoch [16][300/391]	lr: 1.000e-01, eta: 2:07:11, time: 0.133, data_time: 0.000, memory: 2300, loss: 2.0837
2022-11-11 16:40:23,634 - mmcls - INFO - Epoch(val) [16][79]	accuracy_top-1: 32.5400, accuracy_top-5: 63.4600
2022-11-11 16:40:38,726 - mmcls - INFO - Epoch [17][100/391]	lr: 1.000e-01, eta: 2:05:53, time: 0.151, data_time: 0.021, memory: 2300, loss: 2.0447
2022-11-11 16:40:51,816 - mmcls - INFO - Epoch [17][200/391]	lr: 1.000e-01, eta: 2:06:11, time: 0.131, data_time: 0.000, memory: 2300, loss: 2.0200
2022-11-11 16:41:04,880 - mmcls - INFO - Epoch [17][300/391]	lr: 1.000e-01, eta: 2:06:28, time: 0.131, data_time: 0.000, memory: 2300, loss: 2.0213
2022-11-11 16:41:19,660 - mmcls - INFO - Epoch(val) [17][79]	accuracy_top-1: 26.4400, accuracy_top-5: 57.0700
2022-11-11 16:41:34,585 - mmcls - INFO - Epoch [18][100/391]	lr: 1.000e-01, eta: 2:05:11, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.9899
2022-11-11 16:41:47,755 - mmcls - INFO - Epoch [18][200/391]	lr: 1.000e-01, eta: 2:05:28, time: 0.132, data_time: 0.000, memory: 2300, loss: 2.0158
2022-11-11 16:42:00,740 - mmcls - INFO - Epoch [18][300/391]	lr: 1.000e-01, eta: 2:05:43, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.9814
2022-11-11 16:42:15,926 - mmcls - INFO - Epoch(val) [18][79]	accuracy_top-1: 29.9600, accuracy_top-5: 59.8200
2022-11-11 16:42:30,965 - mmcls - INFO - Epoch [19][100/391]	lr: 1.000e-01, eta: 2:04:31, time: 0.150, data_time: 0.021, memory: 2300, loss: 1.9621
2022-11-11 16:42:43,783 - mmcls - INFO - Epoch [19][200/391]	lr: 1.000e-01, eta: 2:04:43, time: 0.128, data_time: 0.000, memory: 2300, loss: 2.0079
2022-11-11 16:42:56,707 - mmcls - INFO - Epoch [19][300/391]	lr: 1.000e-01, eta: 2:04:55, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.9534
2022-11-11 16:43:11,396 - mmcls - INFO - Epoch(val) [19][79]	accuracy_top-1: 28.7700, accuracy_top-5: 58.5000
2022-11-11 16:43:26,371 - mmcls - INFO - Epoch [20][100/391]	lr: 1.000e-01, eta: 2:03:46, time: 0.150, data_time: 0.021, memory: 2300, loss: 1.8945
2022-11-11 16:43:39,234 - mmcls - INFO - Epoch [20][200/391]	lr: 1.000e-01, eta: 2:03:57, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.9460
2022-11-11 16:43:51,928 - mmcls - INFO - Epoch [20][300/391]	lr: 1.000e-01, eta: 2:04:06, time: 0.127, data_time: 0.000, memory: 2300, loss: 1.9719
2022-11-11 16:44:03,822 - mmcls - INFO - Saving checkpoint at 20 epochs
2022-11-11 16:44:06,521 - mmcls - INFO - Epoch(val) [20][79]	accuracy_top-1: 28.5700, accuracy_top-5: 57.6500
2022-11-11 16:44:21,687 - mmcls - INFO - Epoch [21][100/391]	lr: 1.000e-01, eta: 2:03:01, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.9025
2022-11-11 16:44:35,027 - mmcls - INFO - Epoch [21][200/391]	lr: 1.000e-01, eta: 2:03:16, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.9272
2022-11-11 16:44:48,326 - mmcls - INFO - Epoch [21][300/391]	lr: 1.000e-01, eta: 2:03:29, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.9511
2022-11-11 16:45:02,987 - mmcls - INFO - Epoch(val) [21][79]	accuracy_top-1: 34.1100, accuracy_top-5: 65.7800
2022-11-11 16:45:18,052 - mmcls - INFO - Epoch [22][100/391]	lr: 1.000e-01, eta: 2:02:26, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.9088
2022-11-11 16:45:30,960 - mmcls - INFO - Epoch [22][200/391]	lr: 1.000e-01, eta: 2:02:35, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.8865
2022-11-11 16:45:43,844 - mmcls - INFO - Epoch [22][300/391]	lr: 1.000e-01, eta: 2:02:44, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.9029
2022-11-11 16:45:58,143 - mmcls - INFO - Epoch(val) [22][79]	accuracy_top-1: 38.1300, accuracy_top-5: 68.8700
2022-11-11 16:46:13,345 - mmcls - INFO - Epoch [23][100/391]	lr: 1.000e-01, eta: 2:01:44, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.8854
2022-11-11 16:46:26,938 - mmcls - INFO - Epoch [23][200/391]	lr: 1.000e-01, eta: 2:01:57, time: 0.136, data_time: 0.000, memory: 2300, loss: 1.9026
2022-11-11 16:46:40,036 - mmcls - INFO - Epoch [23][300/391]	lr: 1.000e-01, eta: 2:02:07, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.8903
2022-11-11 16:46:54,425 - mmcls - INFO - Epoch(val) [23][79]	accuracy_top-1: 33.6200, accuracy_top-5: 64.9100
2022-11-11 16:47:09,347 - mmcls - INFO - Epoch [24][100/391]	lr: 1.000e-01, eta: 2:01:06, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.8067
2022-11-11 16:47:22,539 - mmcls - INFO - Epoch [24][200/391]	lr: 1.000e-01, eta: 2:01:16, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.8496
2022-11-11 16:47:35,708 - mmcls - INFO - Epoch [24][300/391]	lr: 1.000e-01, eta: 2:01:25, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.8774
2022-11-11 16:47:50,651 - mmcls - INFO - Epoch(val) [24][79]	accuracy_top-1: 36.9600, accuracy_top-5: 69.4000
2022-11-11 16:48:05,918 - mmcls - INFO - Epoch [25][100/391]	lr: 1.000e-01, eta: 2:00:29, time: 0.153, data_time: 0.021, memory: 2300, loss: 1.8581
2022-11-11 16:48:19,129 - mmcls - INFO - Epoch [25][200/391]	lr: 1.000e-01, eta: 2:00:37, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.8495
2022-11-11 16:48:32,397 - mmcls - INFO - Epoch [25][300/391]	lr: 1.000e-01, eta: 2:00:46, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.8290
2022-11-11 16:48:47,116 - mmcls - INFO - Epoch(val) [25][79]	accuracy_top-1: 37.3000, accuracy_top-5: 70.0500
2022-11-11 16:49:02,531 - mmcls - INFO - Epoch [26][100/391]	lr: 1.000e-01, eta: 1:59:53, time: 0.154, data_time: 0.021, memory: 2300, loss: 1.7966
2022-11-11 16:49:16,021 - mmcls - INFO - Epoch [26][200/391]	lr: 1.000e-01, eta: 2:00:02, time: 0.135, data_time: 0.001, memory: 2300, loss: 1.8015
2022-11-11 16:49:28,955 - mmcls - INFO - Epoch [26][300/391]	lr: 1.000e-01, eta: 2:00:08, time: 0.129, data_time: 0.001, memory: 2300, loss: 1.8444
2022-11-11 16:49:43,552 - mmcls - INFO - Epoch(val) [26][79]	accuracy_top-1: 33.7800, accuracy_top-5: 63.7200
2022-11-11 16:49:58,694 - mmcls - INFO - Epoch [27][100/391]	lr: 1.000e-01, eta: 1:59:14, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.7900
2022-11-11 16:50:12,049 - mmcls - INFO - Epoch [27][200/391]	lr: 1.000e-01, eta: 1:59:22, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.7820
2022-11-11 16:50:25,571 - mmcls - INFO - Epoch [27][300/391]	lr: 1.000e-01, eta: 1:59:31, time: 0.135, data_time: 0.000, memory: 2300, loss: 1.8190
2022-11-11 16:50:40,055 - mmcls - INFO - Epoch(val) [27][79]	accuracy_top-1: 34.3300, accuracy_top-5: 65.8300
2022-11-11 16:50:54,987 - mmcls - INFO - Epoch [28][100/391]	lr: 1.000e-01, eta: 1:58:37, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.7859
2022-11-11 16:51:08,177 - mmcls - INFO - Epoch [28][200/391]	lr: 1.000e-01, eta: 1:58:43, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.8342
2022-11-11 16:51:21,284 - mmcls - INFO - Epoch [28][300/391]	lr: 1.000e-01, eta: 1:58:48, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.8093
2022-11-11 16:51:35,679 - mmcls - INFO - Epoch(val) [28][79]	accuracy_top-1: 30.7400, accuracy_top-5: 62.4100
2022-11-11 16:51:50,339 - mmcls - INFO - Epoch [29][100/391]	lr: 1.000e-01, eta: 1:57:54, time: 0.146, data_time: 0.021, memory: 2300, loss: 1.7342
2022-11-11 16:52:03,195 - mmcls - INFO - Epoch [29][200/391]	lr: 1.000e-01, eta: 1:57:58, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.7859
2022-11-11 16:52:16,415 - mmcls - INFO - Epoch [29][300/391]	lr: 1.000e-01, eta: 1:58:03, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.8007
2022-11-11 16:52:31,165 - mmcls - INFO - Epoch(val) [29][79]	accuracy_top-1: 44.3400, accuracy_top-5: 75.0800
2022-11-11 16:52:46,242 - mmcls - INFO - Epoch [30][100/391]	lr: 1.000e-01, eta: 1:57:13, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.7248
2022-11-11 16:52:59,371 - mmcls - INFO - Epoch [30][200/391]	lr: 1.000e-01, eta: 1:57:17, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.7302
2022-11-11 16:53:12,313 - mmcls - INFO - Epoch [30][300/391]	lr: 1.000e-01, eta: 1:57:20, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.7937
2022-11-11 16:53:24,137 - mmcls - INFO - Saving checkpoint at 30 epochs
2022-11-11 16:53:26,861 - mmcls - INFO - Epoch(val) [30][79]	accuracy_top-1: 38.1900, accuracy_top-5: 70.2300
2022-11-11 16:53:42,249 - mmcls - INFO - Epoch [31][100/391]	lr: 1.000e-01, eta: 1:56:33, time: 0.154, data_time: 0.021, memory: 2300, loss: 1.7192
2022-11-11 16:53:55,370 - mmcls - INFO - Epoch [31][200/391]	lr: 1.000e-01, eta: 1:56:37, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.7305
2022-11-11 16:54:08,445 - mmcls - INFO - Epoch [31][300/391]	lr: 1.000e-01, eta: 1:56:40, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.7462
2022-11-11 16:54:22,929 - mmcls - INFO - Epoch(val) [31][79]	accuracy_top-1: 38.7900, accuracy_top-5: 70.5100
2022-11-11 16:54:37,763 - mmcls - INFO - Epoch [32][100/391]	lr: 1.000e-01, eta: 1:55:51, time: 0.148, data_time: 0.021, memory: 2300, loss: 1.7128
2022-11-11 16:54:50,421 - mmcls - INFO - Epoch [32][200/391]	lr: 1.000e-01, eta: 1:55:52, time: 0.127, data_time: 0.000, memory: 2300, loss: 1.7032
2022-11-11 16:55:03,286 - mmcls - INFO - Epoch [32][300/391]	lr: 1.000e-01, eta: 1:55:53, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.7795
2022-11-11 16:55:17,542 - mmcls - INFO - Epoch(val) [32][79]	accuracy_top-1: 30.5000, accuracy_top-5: 59.6900
2022-11-11 16:55:32,278 - mmcls - INFO - Epoch [33][100/391]	lr: 1.000e-01, eta: 1:55:05, time: 0.147, data_time: 0.021, memory: 2300, loss: 1.7254
2022-11-11 16:55:44,954 - mmcls - INFO - Epoch [33][200/391]	lr: 1.000e-01, eta: 1:55:05, time: 0.127, data_time: 0.000, memory: 2300, loss: 1.6949
2022-11-11 16:55:58,144 - mmcls - INFO - Epoch [33][300/391]	lr: 1.000e-01, eta: 1:55:08, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.7219
2022-11-11 16:56:12,671 - mmcls - INFO - Epoch(val) [33][79]	accuracy_top-1: 37.5800, accuracy_top-5: 68.3400
2022-11-11 16:56:27,619 - mmcls - INFO - Epoch [34][100/391]	lr: 1.000e-01, eta: 1:54:22, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.6853
2022-11-11 16:56:40,583 - mmcls - INFO - Epoch [34][200/391]	lr: 1.000e-01, eta: 1:54:23, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.6933
2022-11-11 16:56:53,538 - mmcls - INFO - Epoch [34][300/391]	lr: 1.000e-01, eta: 1:54:25, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.7259
2022-11-11 16:57:07,731 - mmcls - INFO - Epoch(val) [34][79]	accuracy_top-1: 45.3800, accuracy_top-5: 76.6000
2022-11-11 16:57:22,282 - mmcls - INFO - Epoch [35][100/391]	lr: 1.000e-01, eta: 1:53:37, time: 0.145, data_time: 0.021, memory: 2300, loss: 1.6763
2022-11-11 16:57:35,278 - mmcls - INFO - Epoch [35][200/391]	lr: 1.000e-01, eta: 1:53:38, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.6856
2022-11-11 16:57:48,229 - mmcls - INFO - Epoch [35][300/391]	lr: 1.000e-01, eta: 1:53:39, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.6913
2022-11-11 16:58:02,543 - mmcls - INFO - Epoch(val) [35][79]	accuracy_top-1: 32.2100, accuracy_top-5: 61.5700
2022-11-11 16:58:18,048 - mmcls - INFO - Epoch [36][100/391]	lr: 1.000e-01, eta: 1:52:57, time: 0.155, data_time: 0.021, memory: 2300, loss: 1.6798
2022-11-11 16:58:31,334 - mmcls - INFO - Epoch [36][200/391]	lr: 1.000e-01, eta: 1:53:00, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.6905
2022-11-11 16:58:44,481 - mmcls - INFO - Epoch [36][300/391]	lr: 1.000e-01, eta: 1:53:01, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.7042
2022-11-11 16:58:59,131 - mmcls - INFO - Epoch(val) [36][79]	accuracy_top-1: 38.4900, accuracy_top-5: 69.3400
2022-11-11 16:59:14,308 - mmcls - INFO - Epoch [37][100/391]	lr: 1.000e-01, eta: 1:52:18, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.6651
2022-11-11 16:59:27,845 - mmcls - INFO - Epoch [37][200/391]	lr: 1.000e-01, eta: 1:52:21, time: 0.135, data_time: 0.000, memory: 2300, loss: 1.6511
2022-11-11 16:59:40,826 - mmcls - INFO - Epoch [37][300/391]	lr: 1.000e-01, eta: 1:52:22, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.7142
2022-11-11 16:59:55,130 - mmcls - INFO - Epoch(val) [37][79]	accuracy_top-1: 30.0600, accuracy_top-5: 61.4300
2022-11-11 17:00:10,321 - mmcls - INFO - Epoch [38][100/391]	lr: 1.000e-01, eta: 1:51:40, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.6484
2022-11-11 17:00:23,302 - mmcls - INFO - Epoch [38][200/391]	lr: 1.000e-01, eta: 1:51:40, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.6579
2022-11-11 17:00:36,323 - mmcls - INFO - Epoch [38][300/391]	lr: 1.000e-01, eta: 1:51:40, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.6996
2022-11-11 17:00:50,773 - mmcls - INFO - Epoch(val) [38][79]	accuracy_top-1: 34.8300, accuracy_top-5: 63.4400
2022-11-11 17:01:05,634 - mmcls - INFO - Epoch [39][100/391]	lr: 1.000e-01, eta: 1:50:57, time: 0.148, data_time: 0.021, memory: 2300, loss: 1.6165
2022-11-11 17:01:18,453 - mmcls - INFO - Epoch [39][200/391]	lr: 1.000e-01, eta: 1:50:56, time: 0.128, data_time: 0.000, memory: 2300, loss: 1.6862
2022-11-11 17:01:31,303 - mmcls - INFO - Epoch [39][300/391]	lr: 1.000e-01, eta: 1:50:55, time: 0.128, data_time: 0.000, memory: 2300, loss: 1.6669
2022-11-11 17:01:45,489 - mmcls - INFO - Epoch(val) [39][79]	accuracy_top-1: 33.9800, accuracy_top-5: 63.9400
2022-11-11 17:02:00,296 - mmcls - INFO - Epoch [40][100/391]	lr: 1.000e-01, eta: 1:50:13, time: 0.148, data_time: 0.021, memory: 2300, loss: 1.6070
2022-11-11 17:02:13,569 - mmcls - INFO - Epoch [40][200/391]	lr: 1.000e-01, eta: 1:50:14, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.6793
2022-11-11 17:02:26,707 - mmcls - INFO - Epoch [40][300/391]	lr: 1.000e-01, eta: 1:50:14, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.6476
2022-11-11 17:02:38,303 - mmcls - INFO - Saving checkpoint at 40 epochs
2022-11-11 17:02:40,978 - mmcls - INFO - Epoch(val) [40][79]	accuracy_top-1: 44.9600, accuracy_top-5: 76.0300
2022-11-11 17:02:55,705 - mmcls - INFO - Epoch [41][100/391]	lr: 1.000e-01, eta: 1:49:32, time: 0.147, data_time: 0.021, memory: 2300, loss: 1.6313
2022-11-11 17:03:08,843 - mmcls - INFO - Epoch [41][200/391]	lr: 1.000e-01, eta: 1:49:32, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.6336
2022-11-11 17:03:21,765 - mmcls - INFO - Epoch [41][300/391]	lr: 1.000e-01, eta: 1:49:30, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.6310
2022-11-11 17:03:36,131 - mmcls - INFO - Epoch(val) [41][79]	accuracy_top-1: 34.8400, accuracy_top-5: 66.5900
2022-11-11 17:03:51,353 - mmcls - INFO - Epoch [42][100/391]	lr: 1.000e-01, eta: 1:48:51, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.6043
2022-11-11 17:04:04,430 - mmcls - INFO - Epoch [42][200/391]	lr: 1.000e-01, eta: 1:48:50, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.6054
2022-11-11 17:04:18,311 - mmcls - INFO - Epoch [42][300/391]	lr: 1.000e-01, eta: 1:48:52, time: 0.139, data_time: 0.001, memory: 2300, loss: 1.6187
2022-11-11 17:04:32,878 - mmcls - INFO - Epoch(val) [42][79]	accuracy_top-1: 33.3100, accuracy_top-5: 64.4800
2022-11-11 17:04:47,789 - mmcls - INFO - Epoch [43][100/391]	lr: 1.000e-01, eta: 1:48:13, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.6082
2022-11-11 17:05:00,738 - mmcls - INFO - Epoch [43][200/391]	lr: 1.000e-01, eta: 1:48:11, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.6280
2022-11-11 17:05:13,718 - mmcls - INFO - Epoch [43][300/391]	lr: 1.000e-01, eta: 1:48:09, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.6220
2022-11-11 17:05:28,198 - mmcls - INFO - Epoch(val) [43][79]	accuracy_top-1: 29.0900, accuracy_top-5: 58.0600
2022-11-11 17:05:43,566 - mmcls - INFO - Epoch [44][100/391]	lr: 1.000e-01, eta: 1:47:32, time: 0.154, data_time: 0.021, memory: 2300, loss: 1.5718
2022-11-11 17:05:57,007 - mmcls - INFO - Epoch [44][200/391]	lr: 1.000e-01, eta: 1:47:32, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.6329
2022-11-11 17:06:10,709 - mmcls - INFO - Epoch [44][300/391]	lr: 1.000e-01, eta: 1:47:33, time: 0.137, data_time: 0.000, memory: 2300, loss: 1.6264
2022-11-11 17:06:25,952 - mmcls - INFO - Epoch(val) [44][79]	accuracy_top-1: 29.2300, accuracy_top-5: 57.2500
2022-11-11 17:06:40,881 - mmcls - INFO - Epoch [45][100/391]	lr: 1.000e-01, eta: 1:46:54, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.5690
2022-11-11 17:06:54,169 - mmcls - INFO - Epoch [45][200/391]	lr: 1.000e-01, eta: 1:46:53, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.5863
2022-11-11 17:07:07,549 - mmcls - INFO - Epoch [45][300/391]	lr: 1.000e-01, eta: 1:46:52, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.6254
2022-11-11 17:07:22,498 - mmcls - INFO - Epoch(val) [45][79]	accuracy_top-1: 41.8400, accuracy_top-5: 73.7700
2022-11-11 17:07:37,785 - mmcls - INFO - Epoch [46][100/391]	lr: 1.000e-01, eta: 1:46:15, time: 0.153, data_time: 0.021, memory: 2300, loss: 1.5449
2022-11-11 17:07:50,837 - mmcls - INFO - Epoch [46][200/391]	lr: 1.000e-01, eta: 1:46:13, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.5849
2022-11-11 17:08:04,012 - mmcls - INFO - Epoch [46][300/391]	lr: 1.000e-01, eta: 1:46:12, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.6516
2022-11-11 17:08:18,834 - mmcls - INFO - Epoch(val) [46][79]	accuracy_top-1: 38.7400, accuracy_top-5: 69.1200
2022-11-11 17:08:33,891 - mmcls - INFO - Epoch [47][100/391]	lr: 1.000e-01, eta: 1:45:34, time: 0.150, data_time: 0.021, memory: 2300, loss: 1.5386
2022-11-11 17:08:47,010 - mmcls - INFO - Epoch [47][200/391]	lr: 1.000e-01, eta: 1:45:32, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.5854
2022-11-11 17:09:00,057 - mmcls - INFO - Epoch [47][300/391]	lr: 1.000e-01, eta: 1:45:30, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.5901
2022-11-11 17:09:14,679 - mmcls - INFO - Epoch(val) [47][79]	accuracy_top-1: 41.1000, accuracy_top-5: 71.2800
2022-11-11 17:09:29,630 - mmcls - INFO - Epoch [48][100/391]	lr: 1.000e-01, eta: 1:44:53, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.5840
2022-11-11 17:09:42,915 - mmcls - INFO - Epoch [48][200/391]	lr: 1.000e-01, eta: 1:44:51, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.5515
2022-11-11 17:09:56,140 - mmcls - INFO - Epoch [48][300/391]	lr: 1.000e-01, eta: 1:44:49, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.5637
2022-11-11 17:10:11,173 - mmcls - INFO - Epoch(val) [48][79]	accuracy_top-1: 28.2400, accuracy_top-5: 58.3400
2022-11-11 17:10:26,837 - mmcls - INFO - Epoch [49][100/391]	lr: 1.000e-01, eta: 1:44:15, time: 0.157, data_time: 0.021, memory: 2300, loss: 1.5531
2022-11-11 17:10:40,214 - mmcls - INFO - Epoch [49][200/391]	lr: 1.000e-01, eta: 1:44:13, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.5429
2022-11-11 17:10:53,269 - mmcls - INFO - Epoch [49][300/391]	lr: 1.000e-01, eta: 1:44:10, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.5631
2022-11-11 17:11:08,217 - mmcls - INFO - Epoch(val) [49][79]	accuracy_top-1: 44.1800, accuracy_top-5: 74.1400
2022-11-11 17:11:23,315 - mmcls - INFO - Epoch [50][100/391]	lr: 1.000e-01, eta: 1:43:34, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.5063
2022-11-11 17:11:36,292 - mmcls - INFO - Epoch [50][200/391]	lr: 1.000e-01, eta: 1:43:31, time: 0.130, data_time: 0.001, memory: 2300, loss: 1.5813
2022-11-11 17:11:49,288 - mmcls - INFO - Epoch [50][300/391]	lr: 1.000e-01, eta: 1:43:28, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.5518
2022-11-11 17:12:01,444 - mmcls - INFO - Saving checkpoint at 50 epochs
2022-11-11 17:12:04,265 - mmcls - INFO - Epoch(val) [50][79]	accuracy_top-1: 39.3100, accuracy_top-5: 70.7900
2022-11-11 17:12:19,512 - mmcls - INFO - Epoch [51][100/391]	lr: 1.000e-01, eta: 1:42:53, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.4879
2022-11-11 17:12:32,883 - mmcls - INFO - Epoch [51][200/391]	lr: 1.000e-01, eta: 1:42:51, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.5491
2022-11-11 17:12:46,161 - mmcls - INFO - Epoch [51][300/391]	lr: 1.000e-01, eta: 1:42:48, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.5600
2022-11-11 17:13:00,842 - mmcls - INFO - Epoch(val) [51][79]	accuracy_top-1: 41.9500, accuracy_top-5: 73.2400
2022-11-11 17:13:15,870 - mmcls - INFO - Epoch [52][100/391]	lr: 1.000e-01, eta: 1:42:13, time: 0.150, data_time: 0.021, memory: 2300, loss: 1.4984
2022-11-11 17:13:29,038 - mmcls - INFO - Epoch [52][200/391]	lr: 1.000e-01, eta: 1:42:10, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.5165
2022-11-11 17:13:42,211 - mmcls - INFO - Epoch [52][300/391]	lr: 1.000e-01, eta: 1:42:07, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.5689
2022-11-11 17:13:57,971 - mmcls - INFO - Epoch(val) [52][79]	accuracy_top-1: 44.4000, accuracy_top-5: 76.2500
2022-11-11 17:14:13,942 - mmcls - INFO - Epoch [53][100/391]	lr: 1.000e-01, eta: 1:41:35, time: 0.160, data_time: 0.021, memory: 2300, loss: 1.5202
2022-11-11 17:14:27,644 - mmcls - INFO - Epoch [53][200/391]	lr: 1.000e-01, eta: 1:41:33, time: 0.137, data_time: 0.000, memory: 2300, loss: 1.5188
2022-11-11 17:14:40,978 - mmcls - INFO - Epoch [53][300/391]	lr: 1.000e-01, eta: 1:41:30, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.5432
2022-11-11 17:14:56,134 - mmcls - INFO - Epoch(val) [53][79]	accuracy_top-1: 35.9100, accuracy_top-5: 64.8900
2022-11-11 17:15:11,608 - mmcls - INFO - Epoch [54][100/391]	lr: 1.000e-01, eta: 1:40:57, time: 0.155, data_time: 0.021, memory: 2300, loss: 1.4696
2022-11-11 17:15:24,998 - mmcls - INFO - Epoch [54][200/391]	lr: 1.000e-01, eta: 1:40:54, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.5249
2022-11-11 17:15:38,402 - mmcls - INFO - Epoch [54][300/391]	lr: 1.000e-01, eta: 1:40:51, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.5301
2022-11-11 17:15:53,174 - mmcls - INFO - Epoch(val) [54][79]	accuracy_top-1: 39.7300, accuracy_top-5: 70.5300
2022-11-11 17:16:08,557 - mmcls - INFO - Epoch [55][100/391]	lr: 1.000e-01, eta: 1:40:18, time: 0.154, data_time: 0.021, memory: 2300, loss: 1.4838
2022-11-11 17:16:21,682 - mmcls - INFO - Epoch [55][200/391]	lr: 1.000e-01, eta: 1:40:14, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.5193
2022-11-11 17:16:35,038 - mmcls - INFO - Epoch [55][300/391]	lr: 1.000e-01, eta: 1:40:11, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.5075
2022-11-11 17:16:49,813 - mmcls - INFO - Epoch(val) [55][79]	accuracy_top-1: 35.5500, accuracy_top-5: 66.2100
2022-11-11 17:17:04,885 - mmcls - INFO - Epoch [56][100/391]	lr: 1.000e-01, eta: 1:39:37, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.4942
2022-11-11 17:17:17,833 - mmcls - INFO - Epoch [56][200/391]	lr: 1.000e-01, eta: 1:39:33, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.5160
2022-11-11 17:17:30,768 - mmcls - INFO - Epoch [56][300/391]	lr: 1.000e-01, eta: 1:39:29, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.5108
2022-11-11 17:17:45,929 - mmcls - INFO - Epoch(val) [56][79]	accuracy_top-1: 36.5900, accuracy_top-5: 67.4500
2022-11-11 17:18:01,453 - mmcls - INFO - Epoch [57][100/391]	lr: 1.000e-01, eta: 1:38:56, time: 0.155, data_time: 0.021, memory: 2300, loss: 1.4821
2022-11-11 17:18:15,015 - mmcls - INFO - Epoch [57][200/391]	lr: 1.000e-01, eta: 1:38:53, time: 0.136, data_time: 0.000, memory: 2300, loss: 1.4885
2022-11-11 17:18:28,521 - mmcls - INFO - Epoch [57][300/391]	lr: 1.000e-01, eta: 1:38:50, time: 0.135, data_time: 0.000, memory: 2300, loss: 1.5276
2022-11-11 17:18:42,981 - mmcls - INFO - Epoch(val) [57][79]	accuracy_top-1: 23.4800, accuracy_top-5: 46.1300
2022-11-11 17:18:57,813 - mmcls - INFO - Epoch [58][100/391]	lr: 1.000e-01, eta: 1:38:16, time: 0.148, data_time: 0.021, memory: 2300, loss: 1.5038
2022-11-11 17:19:11,175 - mmcls - INFO - Epoch [58][200/391]	lr: 1.000e-01, eta: 1:38:13, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.4920
2022-11-11 17:19:24,473 - mmcls - INFO - Epoch [58][300/391]	lr: 1.000e-01, eta: 1:38:09, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.4618
2022-11-11 17:19:39,324 - mmcls - INFO - Epoch(val) [58][79]	accuracy_top-1: 39.2800, accuracy_top-5: 71.5500
2022-11-11 17:19:54,029 - mmcls - INFO - Epoch [59][100/391]	lr: 1.000e-01, eta: 1:37:35, time: 0.147, data_time: 0.021, memory: 2300, loss: 1.4575
2022-11-11 17:20:07,189 - mmcls - INFO - Epoch [59][200/391]	lr: 1.000e-01, eta: 1:37:31, time: 0.132, data_time: 0.000, memory: 2300, loss: 1.4948
2022-11-11 17:20:20,267 - mmcls - INFO - Epoch [59][300/391]	lr: 1.000e-01, eta: 1:37:26, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.5234
2022-11-11 17:20:35,091 - mmcls - INFO - Epoch(val) [59][79]	accuracy_top-1: 40.9700, accuracy_top-5: 73.0100
2022-11-11 17:20:50,089 - mmcls - INFO - Epoch [60][100/391]	lr: 1.000e-01, eta: 1:36:54, time: 0.150, data_time: 0.021, memory: 2300, loss: 1.4376
2022-11-11 17:21:03,055 - mmcls - INFO - Epoch [60][200/391]	lr: 1.000e-01, eta: 1:36:49, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.4882
2022-11-11 17:21:15,987 - mmcls - INFO - Epoch [60][300/391]	lr: 1.000e-01, eta: 1:36:44, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.5111
2022-11-11 17:21:27,738 - mmcls - INFO - Saving checkpoint at 60 epochs
2022-11-11 17:21:30,489 - mmcls - INFO - Epoch(val) [60][79]	accuracy_top-1: 31.7200, accuracy_top-5: 59.2400
2022-11-11 17:21:45,511 - mmcls - INFO - Epoch [61][100/391]	lr: 1.000e-01, eta: 1:36:11, time: 0.150, data_time: 0.021, memory: 2300, loss: 1.4246
2022-11-11 17:21:58,767 - mmcls - INFO - Epoch [61][200/391]	lr: 1.000e-01, eta: 1:36:07, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.4791
2022-11-11 17:22:12,227 - mmcls - INFO - Epoch [61][300/391]	lr: 1.000e-01, eta: 1:36:03, time: 0.135, data_time: 0.000, memory: 2300, loss: 1.4916
2022-11-11 17:22:27,203 - mmcls - INFO - Epoch(val) [61][79]	accuracy_top-1: 46.5900, accuracy_top-5: 77.0000
2022-11-11 17:22:41,908 - mmcls - INFO - Epoch [62][100/391]	lr: 1.000e-01, eta: 1:35:30, time: 0.147, data_time: 0.021, memory: 2300, loss: 1.4212
2022-11-11 17:22:55,037 - mmcls - INFO - Epoch [62][200/391]	lr: 1.000e-01, eta: 1:35:26, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.4984
2022-11-11 17:23:08,607 - mmcls - INFO - Epoch [62][300/391]	lr: 1.000e-01, eta: 1:35:22, time: 0.136, data_time: 0.000, memory: 2300, loss: 1.5076
2022-11-11 17:23:23,374 - mmcls - INFO - Epoch(val) [62][79]	accuracy_top-1: 38.3600, accuracy_top-5: 71.8900
2022-11-11 17:23:38,479 - mmcls - INFO - Epoch [63][100/391]	lr: 1.000e-01, eta: 1:34:50, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.4254
2022-11-11 17:23:51,580 - mmcls - INFO - Epoch [63][200/391]	lr: 1.000e-01, eta: 1:34:45, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.4569
2022-11-11 17:24:04,622 - mmcls - INFO - Epoch [63][300/391]	lr: 1.000e-01, eta: 1:34:40, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.4805
2022-11-11 17:24:19,315 - mmcls - INFO - Epoch(val) [63][79]	accuracy_top-1: 35.0300, accuracy_top-5: 65.5600
2022-11-11 17:24:34,300 - mmcls - INFO - Epoch [64][100/391]	lr: 1.000e-01, eta: 1:34:08, time: 0.150, data_time: 0.021, memory: 2300, loss: 1.4036
2022-11-11 17:24:47,243 - mmcls - INFO - Epoch [64][200/391]	lr: 1.000e-01, eta: 1:34:03, time: 0.129, data_time: 0.000, memory: 2300, loss: 1.4505
2022-11-11 17:25:00,270 - mmcls - INFO - Epoch [64][300/391]	lr: 1.000e-01, eta: 1:33:58, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.4712
2022-11-11 17:25:15,000 - mmcls - INFO - Epoch(val) [64][79]	accuracy_top-1: 46.1500, accuracy_top-5: 76.1700
2022-11-11 17:25:29,866 - mmcls - INFO - Epoch [65][100/391]	lr: 1.000e-01, eta: 1:33:26, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.4202
2022-11-11 17:25:42,863 - mmcls - INFO - Epoch [65][200/391]	lr: 1.000e-01, eta: 1:33:21, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.4768
2022-11-11 17:25:55,844 - mmcls - INFO - Epoch [65][300/391]	lr: 1.000e-01, eta: 1:33:15, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.4568
2022-11-11 17:26:11,058 - mmcls - INFO - Epoch(val) [65][79]	accuracy_top-1: 37.6000, accuracy_top-5: 67.0000
2022-11-11 17:26:26,457 - mmcls - INFO - Epoch [66][100/391]	lr: 1.000e-01, eta: 1:32:45, time: 0.154, data_time: 0.021, memory: 2300, loss: 1.4257
2022-11-11 17:26:39,590 - mmcls - INFO - Epoch [66][200/391]	lr: 1.000e-01, eta: 1:32:40, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.4731
2022-11-11 17:26:52,859 - mmcls - INFO - Epoch [66][300/391]	lr: 1.000e-01, eta: 1:32:35, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.4561
2022-11-11 17:27:07,757 - mmcls - INFO - Epoch(val) [66][79]	accuracy_top-1: 40.3600, accuracy_top-5: 70.9300
2022-11-11 17:27:22,972 - mmcls - INFO - Epoch [67][100/391]	lr: 1.000e-01, eta: 1:32:04, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.3799
2022-11-11 17:27:36,373 - mmcls - INFO - Epoch [67][200/391]	lr: 1.000e-01, eta: 1:31:59, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.4507
2022-11-11 17:27:49,702 - mmcls - INFO - Epoch [67][300/391]	lr: 1.000e-01, eta: 1:31:54, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.4750
2022-11-11 17:28:04,345 - mmcls - INFO - Epoch(val) [67][79]	accuracy_top-1: 33.2900, accuracy_top-5: 61.8500
2022-11-11 17:28:19,483 - mmcls - INFO - Epoch [68][100/391]	lr: 1.000e-01, eta: 1:31:24, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.3886
2022-11-11 17:28:32,611 - mmcls - INFO - Epoch [68][200/391]	lr: 1.000e-01, eta: 1:31:18, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.4444
2022-11-11 17:28:45,910 - mmcls - INFO - Epoch [68][300/391]	lr: 1.000e-01, eta: 1:31:13, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.4606
2022-11-11 17:29:00,573 - mmcls - INFO - Epoch(val) [68][79]	accuracy_top-1: 35.5000, accuracy_top-5: 67.0400
2022-11-11 17:29:15,655 - mmcls - INFO - Epoch [69][100/391]	lr: 1.000e-01, eta: 1:30:43, time: 0.151, data_time: 0.021, memory: 2300, loss: 1.3925
2022-11-11 17:29:28,691 - mmcls - INFO - Epoch [69][200/391]	lr: 1.000e-01, eta: 1:30:37, time: 0.130, data_time: 0.000, memory: 2300, loss: 1.3934
2022-11-11 17:29:42,106 - mmcls - INFO - Epoch [69][300/391]	lr: 1.000e-01, eta: 1:30:32, time: 0.134, data_time: 0.000, memory: 2300, loss: 1.4562
2022-11-11 17:32:38,316 - mmcls - INFO - Epoch(val) [69][79]	accuracy_top-1: 40.5500, accuracy_top-5: 71.4400
2022-11-11 17:32:51,502 - mmcls - INFO - Epoch [70][100/391]	lr: 1.000e-01, eta: 1:29:59, time: 0.132, data_time: 0.021, memory: 2300, loss: 1.3906
2022-11-11 17:33:02,606 - mmcls - INFO - Epoch [70][200/391]	lr: 1.000e-01, eta: 1:29:49, time: 0.111, data_time: 0.000, memory: 2300, loss: 1.4032
2022-11-11 17:33:13,762 - mmcls - INFO - Epoch [70][300/391]	lr: 1.000e-01, eta: 1:29:40, time: 0.112, data_time: 0.000, memory: 2300, loss: 1.4391
2022-11-11 17:33:24,069 - mmcls - INFO - Saving checkpoint at 70 epochs
2022-11-11 17:33:26,503 - mmcls - INFO - Epoch(val) [70][79]	accuracy_top-1: 36.3100, accuracy_top-5: 65.5700
2022-11-11 17:33:40,530 - mmcls - INFO - Epoch [71][100/391]	lr: 1.000e-01, eta: 1:29:08, time: 0.140, data_time: 0.021, memory: 2300, loss: 1.4043
2022-11-11 17:33:52,773 - mmcls - INFO - Epoch [71][200/391]	lr: 1.000e-01, eta: 1:29:01, time: 0.123, data_time: 0.000, memory: 2300, loss: 1.4453
2022-11-11 17:34:04,993 - mmcls - INFO - Epoch [71][300/391]	lr: 1.000e-01, eta: 1:28:53, time: 0.122, data_time: 0.000, memory: 2300, loss: 1.4499
2022-11-11 17:34:18,653 - mmcls - INFO - Epoch(val) [71][79]	accuracy_top-1: 32.9200, accuracy_top-5: 61.1300
2022-11-11 17:34:32,808 - mmcls - INFO - Epoch [72][100/391]	lr: 1.000e-01, eta: 1:28:22, time: 0.142, data_time: 0.021, memory: 2300, loss: 1.3684
2022-11-11 17:34:44,501 - mmcls - INFO - Epoch [72][200/391]	lr: 1.000e-01, eta: 1:28:14, time: 0.117, data_time: 0.000, memory: 2300, loss: 1.4119
2022-11-11 17:34:56,287 - mmcls - INFO - Epoch [72][300/391]	lr: 1.000e-01, eta: 1:28:05, time: 0.118, data_time: 0.000, memory: 2300, loss: 1.4310
2022-11-11 17:35:10,013 - mmcls - INFO - Epoch(val) [72][79]	accuracy_top-1: 39.0000, accuracy_top-5: 67.7400
2022-11-11 17:35:24,195 - mmcls - INFO - Epoch [73][100/391]	lr: 1.000e-01, eta: 1:27:35, time: 0.142, data_time: 0.021, memory: 2300, loss: 1.3805
2022-11-11 17:35:36,341 - mmcls - INFO - Epoch [73][200/391]	lr: 1.000e-01, eta: 1:27:27, time: 0.121, data_time: 0.000, memory: 2300, loss: 1.4139
2022-11-11 17:35:48,646 - mmcls - INFO - Epoch [73][300/391]	lr: 1.000e-01, eta: 1:27:19, time: 0.123, data_time: 0.000, memory: 2300, loss: 1.4270
2022-11-11 17:36:01,995 - mmcls - INFO - Epoch(val) [73][79]	accuracy_top-1: 36.9700, accuracy_top-5: 68.3500
2022-11-11 17:36:16,390 - mmcls - INFO - Epoch [74][100/391]	lr: 1.000e-01, eta: 1:26:49, time: 0.144, data_time: 0.021, memory: 2300, loss: 1.3817
2022-11-11 17:36:28,524 - mmcls - INFO - Epoch [74][200/391]	lr: 1.000e-01, eta: 1:26:42, time: 0.121, data_time: 0.000, memory: 2300, loss: 1.4120
2022-11-11 17:36:40,625 - mmcls - INFO - Epoch [74][300/391]	lr: 1.000e-01, eta: 1:26:34, time: 0.121, data_time: 0.000, memory: 2300, loss: 1.4543
2022-11-11 17:36:54,692 - mmcls - INFO - Epoch(val) [74][79]	accuracy_top-1: 33.8800, accuracy_top-5: 63.2400
2022-11-11 17:37:08,572 - mmcls - INFO - Epoch [75][100/391]	lr: 1.000e-01, eta: 1:26:03, time: 0.139, data_time: 0.021, memory: 2300, loss: 1.3344
2022-11-11 17:37:20,675 - mmcls - INFO - Epoch [75][200/391]	lr: 1.000e-01, eta: 1:25:55, time: 0.121, data_time: 0.000, memory: 2300, loss: 1.4075
2022-11-11 17:37:32,921 - mmcls - INFO - Epoch [75][300/391]	lr: 1.000e-01, eta: 1:25:48, time: 0.122, data_time: 0.000, memory: 2300, loss: 1.4339
2022-11-11 17:37:46,897 - mmcls - INFO - Epoch(val) [75][79]	accuracy_top-1: 41.1500, accuracy_top-5: 71.3400
2022-11-11 17:38:01,305 - mmcls - INFO - Epoch [76][100/391]	lr: 1.000e-01, eta: 1:25:18, time: 0.144, data_time: 0.021, memory: 2300, loss: 1.3332
2022-11-11 17:38:13,911 - mmcls - INFO - Epoch [76][200/391]	lr: 1.000e-01, eta: 1:25:11, time: 0.126, data_time: 0.000, memory: 2300, loss: 1.4215
2022-11-11 17:38:26,745 - mmcls - INFO - Epoch [76][300/391]	lr: 1.000e-01, eta: 1:25:05, time: 0.128, data_time: 0.000, memory: 2300, loss: 1.4129
2022-11-11 17:38:40,019 - mmcls - INFO - Epoch(val) [76][79]	accuracy_top-1: 42.0200, accuracy_top-5: 73.4400
2022-11-11 17:38:53,897 - mmcls - INFO - Epoch [77][100/391]	lr: 1.000e-01, eta: 1:24:35, time: 0.139, data_time: 0.021, memory: 2300, loss: 1.3527
2022-11-11 17:39:06,000 - mmcls - INFO - Epoch [77][200/391]	lr: 1.000e-01, eta: 1:24:27, time: 0.121, data_time: 0.000, memory: 2300, loss: 1.4064
2022-11-11 17:39:18,353 - mmcls - INFO - Epoch [77][300/391]	lr: 1.000e-01, eta: 1:24:19, time: 0.124, data_time: 0.000, memory: 2300, loss: 1.3712
2022-11-11 17:39:31,943 - mmcls - INFO - Epoch(val) [77][79]	accuracy_top-1: 25.7300, accuracy_top-5: 54.5900
2022-11-11 17:39:45,890 - mmcls - INFO - Epoch [78][100/391]	lr: 1.000e-01, eta: 1:23:49, time: 0.139, data_time: 0.021, memory: 2300, loss: 1.3379
2022-11-11 17:39:58,357 - mmcls - INFO - Epoch [78][200/391]	lr: 1.000e-01, eta: 1:23:42, time: 0.125, data_time: 0.000, memory: 2300, loss: 1.4149
2022-11-11 17:40:10,732 - mmcls - INFO - Epoch [78][300/391]	lr: 1.000e-01, eta: 1:23:34, time: 0.124, data_time: 0.000, memory: 2300, loss: 1.3988
2022-11-11 17:40:24,792 - mmcls - INFO - Epoch(val) [78][79]	accuracy_top-1: 38.9700, accuracy_top-5: 69.7200
2022-11-11 17:40:38,736 - mmcls - INFO - Epoch [79][100/391]	lr: 1.000e-01, eta: 1:23:05, time: 0.139, data_time: 0.021, memory: 2300, loss: 1.3052
2022-11-11 17:40:50,781 - mmcls - INFO - Epoch [79][200/391]	lr: 1.000e-01, eta: 1:22:57, time: 0.120, data_time: 0.000, memory: 2300, loss: 1.3972
2022-11-11 17:41:02,655 - mmcls - INFO - Epoch [79][300/391]	lr: 1.000e-01, eta: 1:22:49, time: 0.119, data_time: 0.000, memory: 2300, loss: 1.4426
2022-11-11 17:41:15,834 - mmcls - INFO - Epoch(val) [79][79]	accuracy_top-1: 41.4400, accuracy_top-5: 71.4200
2022-11-11 17:41:29,661 - mmcls - INFO - Epoch [80][100/391]	lr: 1.000e-01, eta: 1:22:19, time: 0.138, data_time: 0.021, memory: 2300, loss: 1.3263
2022-11-11 17:41:42,246 - mmcls - INFO - Epoch [80][200/391]	lr: 1.000e-01, eta: 1:22:12, time: 0.126, data_time: 0.000, memory: 2300, loss: 1.4098
2022-11-11 17:41:54,807 - mmcls - INFO - Epoch [80][300/391]	lr: 1.000e-01, eta: 1:22:05, time: 0.126, data_time: 0.000, memory: 2300, loss: 1.3947
2022-11-11 17:42:06,071 - mmcls - INFO - Saving checkpoint at 80 epochs
2022-11-11 17:42:08,590 - mmcls - INFO - Epoch(val) [80][79]	accuracy_top-1: 41.7500, accuracy_top-5: 72.3700
2022-11-11 17:42:22,703 - mmcls - INFO - Epoch [81][100/391]	lr: 1.000e-01, eta: 1:21:36, time: 0.141, data_time: 0.021, memory: 2300, loss: 1.3542
2022-11-11 17:42:34,817 - mmcls - INFO - Epoch [81][200/391]	lr: 1.000e-01, eta: 1:21:28, time: 0.121, data_time: 0.000, memory: 2300, loss: 1.3456
2022-11-11 17:42:46,871 - mmcls - INFO - Epoch [81][300/391]	lr: 1.000e-01, eta: 1:21:20, time: 0.121, data_time: 0.000, memory: 2300, loss: 1.4045
2022-11-11 17:43:00,194 - mmcls - INFO - Epoch(val) [81][79]	accuracy_top-1: 41.2300, accuracy_top-5: 71.7600
2022-11-11 17:43:14,625 - mmcls - INFO - Epoch [82][100/391]	lr: 1.000e-01, eta: 1:20:51, time: 0.144, data_time: 0.021, memory: 2300, loss: 1.3303
2022-11-11 17:43:26,965 - mmcls - INFO - Epoch [82][200/391]	lr: 1.000e-01, eta: 1:20:44, time: 0.123, data_time: 0.000, memory: 2300, loss: 1.3938
2022-11-11 17:43:39,414 - mmcls - INFO - Epoch [82][300/391]	lr: 1.000e-01, eta: 1:20:36, time: 0.124, data_time: 0.000, memory: 2300, loss: 1.3783
2022-11-11 17:43:53,344 - mmcls - INFO - Epoch(val) [82][79]	accuracy_top-1: 36.1600, accuracy_top-5: 68.5500
2022-11-11 17:44:07,649 - mmcls - INFO - Epoch [83][100/391]	lr: 1.000e-01, eta: 1:20:08, time: 0.143, data_time: 0.021, memory: 2300, loss: 1.3414
2022-11-11 17:44:20,176 - mmcls - INFO - Epoch [83][200/391]	lr: 1.000e-01, eta: 1:20:01, time: 0.125, data_time: 0.000, memory: 2300, loss: 1.3784
2022-11-11 17:44:32,886 - mmcls - INFO - Epoch [83][300/391]	lr: 1.000e-01, eta: 1:19:53, time: 0.127, data_time: 0.000, memory: 2300, loss: 1.3603
2022-11-11 17:44:47,999 - mmcls - INFO - Epoch(val) [83][79]	accuracy_top-1: 35.5000, accuracy_top-5: 66.0700
2022-11-11 17:45:02,902 - mmcls - INFO - Epoch [84][100/391]	lr: 1.000e-01, eta: 1:19:26, time: 0.149, data_time: 0.021, memory: 2300, loss: 1.3551
2022-11-11 17:45:15,979 - mmcls - INFO - Epoch [84][200/391]	lr: 1.000e-01, eta: 1:19:19, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.3297
2022-11-11 17:45:29,066 - mmcls - INFO - Epoch [84][300/391]	lr: 1.000e-01, eta: 1:19:13, time: 0.131, data_time: 0.000, memory: 2300, loss: 1.3648
2022-11-11 17:50:59,048 - mmcls - INFO - Epoch(val) [84][79]	accuracy_top-1: 39.1600, accuracy_top-5: 70.3800
2022-11-11 17:51:12,850 - mmcls - INFO - Epoch [85][100/391]	lr: 1.000e-01, eta: 1:18:44, time: 0.138, data_time: 0.021, memory: 2300, loss: 1.3054
2022-11-11 17:51:24,635 - mmcls - INFO - Epoch [85][200/391]	lr: 1.000e-01, eta: 1:18:36, time: 0.118, data_time: 0.000, memory: 2300, loss: 1.3678
2022-11-11 17:51:36,473 - mmcls - INFO - Epoch [85][300/391]	lr: 1.000e-01, eta: 1:18:27, time: 0.118, data_time: 0.000, memory: 2300, loss: 1.4058
2022-11-11 17:51:49,840 - mmcls - INFO - Epoch(val) [85][79]	accuracy_top-1: 31.2700, accuracy_top-5: 60.8700
2022-11-11 17:52:04,103 - mmcls - INFO - Epoch [86][100/391]	lr: 1.000e-01, eta: 1:17:59, time: 0.143, data_time: 0.021, memory: 2300, loss: 1.3401
2022-11-11 17:52:16,832 - mmcls - INFO - Epoch [86][200/391]	lr: 1.000e-01, eta: 1:17:52, time: 0.127, data_time: 0.000, memory: 2300, loss: 1.3884
2022-11-11 17:52:30,155 - mmcls - INFO - Epoch [86][300/391]	lr: 1.000e-01, eta: 1:17:45, time: 0.133, data_time: 0.000, memory: 2300, loss: 1.3879
2022-11-11 17:52:45,100 - mmcls - INFO - Epoch(val) [86][79]	accuracy_top-1: 36.0000, accuracy_top-5: 65.2000
2022-11-11 17:53:00,274 - mmcls - INFO - Epoch [87][100/391]	lr: 1.000e-01, eta: 1:17:19, time: 0.152, data_time: 0.021, memory: 2300, loss: 1.3080
2022-11-11 17:53:13,891 - mmcls - INFO - Epoch [87][200/391]	lr: 1.000e-01, eta: 1:17:13, time: 0.136, data_time: 0.000, memory: 2300, loss: 1.3518
